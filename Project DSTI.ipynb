{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. DATA PREPROCESSING\n",
    "This function performs a thorough cleaning and preprocessing of the dataset, essential for consistent and robust model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation and Spark\n",
    "from pyspark.sql import SparkSession, DataFrame, functions as F\n",
    "\n",
    "# Machine Learning and metrics\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_recall_fscore_support, f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# NLP and text processing\n",
    "import spacy\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Deep Learning and model handling\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW, RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "# Hyperparameter tuning\n",
    "import optuna\n",
    "\n",
    "# Utilities and typing\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple, Any\n",
    "from collections import Counter\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(file_path: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Loads the dataset from a specified file path in Parquet format.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path (str): Path to the dataset file.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: Loaded dataset as a Spark DataFrame.\n",
    "    \"\"\"\n",
    "    # Initialize a Spark session\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"NewsClassification\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Load the Parquet file into a Spark DataFrame\n",
    "    dataset = spark.read.parquet(file_path)\n",
    "    \n",
    "    # Show basic information for verification\n",
    "    dataset.printSchema()\n",
    "    print(f\"Loaded dataset with {dataset.count()} records.\")\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean Categories - This step removes special characters, trims whitespaces, and standardizes the format to handle multi-label cases by splitting with a delimiter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_categories(categories: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Cleans and standardizes a list of categories, handling delimiters and removing\n",
    "    special characters. Ensures all categories are formatted consistently.\n",
    "    \n",
    "    Parameters:\n",
    "    - categories (List[str]): List of category strings to clean.\n",
    "    \n",
    "    Returns:\n",
    "    - List[str]: List of cleaned and standardized categories.\n",
    "    \"\"\"\n",
    "    \n",
    "    cleaned_categories = []\n",
    "    \n",
    "    for category in categories:\n",
    "        # Step 1: Remove special characters and standardize formatting\n",
    "        clean_category = re.sub(r'[^\\w\\s]', '', category)  # Remove special characters\n",
    "        clean_category = clean_category.lower().strip()    # Convert to lowercase and trim whitespace\n",
    "        \n",
    "        # Step 2: Handle delimiters within category names\n",
    "        if ',' in clean_category or ';' in clean_category or '/' in clean_category:\n",
    "            # Split on common delimiters (comma, semicolon, or slash)\n",
    "            subcategories = re.split(r'[,/;]', clean_category)\n",
    "            for subcategory in subcategories:\n",
    "                cleaned_subcategory = subcategory.strip()\n",
    "                if cleaned_subcategory:  # Avoid appending empty strings\n",
    "                    cleaned_categories.append(cleaned_subcategory)\n",
    "        else:\n",
    "            # If no delimiter, add the cleaned category as is\n",
    "            cleaned_categories.append(clean_category)\n",
    "    \n",
    "    return cleaned_categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardize Categories - Once cleaned, categories are filtered based on frequency and standardized by mapping synonyms to a unified format. This improves model generalization by reducing redundancy in labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Spacy model ( en_core_web_sm, en_core_web_md  , en_core_web_lg  , en_core_web_trf )\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "\n",
    "def standardize_categories(cleaned_dataset: DataFrame, threshold_percentage: float = 0.005) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Standardizes categories in the dataset based on a percentage frequency threshold and Spacy\n",
    "    category mappings. This involves filtering out infrequent categories and using Spacy for\n",
    "    synonym and abbreviation normalization.\n",
    "    \n",
    "    Parameters:\n",
    "    - cleaned_dataset (DataFrame): Spark DataFrame with a cleaned categories column.\n",
    "    - threshold_percentage (float): Percentage threshold for category frequency (e.g., 0.005 for 0.5%).\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame: Updated DataFrame with standardized categories.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Calculate minimum frequency based on percentage of total records\n",
    "    total_records = cleaned_dataset.count()\n",
    "    min_frequency = int(total_records * threshold_percentage)\n",
    "\n",
    "    # Step 2: Define Spacy-based category mapping function\n",
    "    def map_category(category: str) -> str:\n",
    "        \"\"\"\n",
    "        Uses Spacy's NLP pipeline to find a standardized label for the category by analyzing\n",
    "        synonymy and similarity, returning a normalized label.\n",
    "        \n",
    "        Parameters:\n",
    "        - category (str): Category name to standardize.\n",
    "        \n",
    "        Returns:\n",
    "        - str: Standardized category name.\n",
    "        \"\"\"\n",
    "        doc = nlp(category)\n",
    "        # Extract the root lemma of the category for standardization\n",
    "        root_lemma = doc[0].lemma_ if doc else category\n",
    "        return root_lemma\n",
    "\n",
    "    # Register map_category as a UDF for Spark to apply it on the DataFrame\n",
    "    map_category_udf = F.udf(map_category)\n",
    "\n",
    "    # Apply the Spacy-based mapping UDF to standardize category names\n",
    "    standardized_dataset = cleaned_dataset.withColumn(\n",
    "        \"standardized_category\", map_category_udf(F.col(\"categories\"))\n",
    "    )\n",
    "\n",
    "    # Step 3: Calculate category frequencies to filter out infrequent categories\n",
    "    category_counts = standardized_dataset.groupBy(\"standardized_category\").count()\n",
    "\n",
    "    # Filter categories based on the calculated minimum frequency\n",
    "    frequent_categories = category_counts.filter(F.col(\"count\") >= min_frequency).select(\"standardized_category\")\n",
    "\n",
    "    # Step 4: Join with the original dataset to keep only frequent categories\n",
    "    standardized_dataset = standardized_dataset.join(\n",
    "        frequent_categories, \n",
    "        on=\"standardized_category\", \n",
    "        how=\"inner\"\n",
    "    )\n",
    "\n",
    "    # Drop the original categories column if no longer needed\n",
    "    standardized_dataset = standardized_dataset.drop(\"categories\")\n",
    "\n",
    "    return standardized_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Cleaning - Removes unwanted elements like special characters and extra spaces, enhancing text uniformity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Cleans a given text by removing special characters, HTML tags, converting to \n",
    "    lowercase, and removing extra spaces.\n",
    "    \n",
    "    Parameters:\n",
    "    - text (str): The text to clean.\n",
    "    \n",
    "    Returns:\n",
    "    - str: Cleaned text.\n",
    "    \"\"\"\n",
    "    # Step 1: Remove HTML tags\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "    \n",
    "    # Step 2: Remove special characters (keep alphanumeric and spaces only)\n",
    "    text = re.sub(r'[^A-Za-z0-9\\s]', '', text)\n",
    "    \n",
    "    # Step 3: Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Step 4: Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_category_distribution(categories: List[str]) -> None:\n",
    "    \"\"\"\n",
    "    Prints the distribution of categories for analysis and verification.\n",
    "    \n",
    "    Parameters:\n",
    "    - categories (List[str]): List of categories to analyze.\n",
    "    \n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Step 1: Calculate category frequency using Counter\n",
    "    category_counts = Counter(categories)\n",
    "    \n",
    "    # Step 2: Calculate total number of categories\n",
    "    total_count = sum(category_counts.values())\n",
    "    \n",
    "    # Step 3: Display category distribution as percentages\n",
    "    print(\"Category Distribution:\")\n",
    "    for category, count in category_counts.items():\n",
    "        percentage = (count / total_count) * 100\n",
    "        print(f\"{category}: {count} ({percentage:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(dataset: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Main procedure for data preprocessing, including cleaning text, categories,\n",
    "    combining fields, and standardizing categories. \n",
    "    \n",
    "    Parameters:\n",
    "    - dataset (DataFrame): Spark DataFrame containing raw data.\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame: Processed Spark DataFrame with cleaned and standardized columns.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Clean the categories column using clean_categories function\n",
    "    # Assuming the column name for categories is 'categories' in the dataset\n",
    "    categories_col = dataset.select(\"categories\").rdd.flatMap(lambda x: x).collect()\n",
    "    cleaned_categories = clean_categories(categories_col)\n",
    "    dataset = dataset.withColumn(\"categories\", F.array(*[F.lit(cat) for cat in cleaned_categories]))\n",
    "\n",
    "    # Step 2: Standardize categories based on frequency threshold\n",
    "    standardized_dataset = standardize_categories(dataset)\n",
    "\n",
    "    # Step 3: Clean the text content in 'plain_text' column\n",
    "    # Assuming the column name for the main article content is 'plain_text'\n",
    "    clean_text_udf = F.udf(clean_text, StringType())\n",
    "    standardized_dataset = standardized_dataset.withColumn(\"plain_text\", clean_text_udf(F.col(\"plain_text\")))\n",
    "\n",
    "    # Step 4: Clean the title in 'title' column\n",
    "    # Assuming the column name for title is 'title'\n",
    "    clean_title_udf = F.udf(clean_text, StringType())  # Reuse clean_text function for title\n",
    "    standardized_dataset = standardized_dataset.withColumn(\"title\", clean_title_udf(F.col(\"title\")))\n",
    "\n",
    "    # Step 5: Concatenate title and text into a new column 'full_text' without using UDF\n",
    "    separator = \" [SEP] \"\n",
    "    standardized_dataset = standardized_dataset.withColumn(\n",
    "        \"full_text\",\n",
    "        F.concat_ws(separator, F.col(\"title\"), F.col(\"plain_text\"))\n",
    "    )\n",
    "\n",
    "    # Step 6: Remove duplicates from the dataset directly without a function call\n",
    "    standardized_dataset = standardized_dataset.dropDuplicates()\n",
    "\n",
    "    # Step 7: Print category distribution for analysis\n",
    "    # Assuming we have a list of categories from the cleaned 'categories' column\n",
    "    unique_categories = standardized_dataset.select(\"categories\").rdd.flatMap(lambda x: x).collect()\n",
    "    print_category_distribution(unique_categories)\n",
    "\n",
    "    # Step 8: Drop unnecessary columns if not needed\n",
    "    # Drop 'title' and 'plain_text' if 'full_text' will be used for model training\n",
    "    standardized_dataset = standardized_dataset.drop(\"title\", \"plain_text\")\n",
    "\n",
    "    return standardized_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.TEXT COMPOSITION VALIDATION\n",
    "This function identifies the best text structure for model input, crucial for maximizing classification accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation Set Split - A subset of data is allocated to evaluate different text composition strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_validation_set(dataset: DataFrame, size: float) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Splits the dataset into a small validation set based on the specified size.\n",
    "    \n",
    "    Parameters:\n",
    "    - dataset (DataFrame): Spark DataFrame containing the dataset.\n",
    "    - size (float): Proportion of the dataset to use as the validation set (e.g., 0.1 for 10%).\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame: Spark DataFrame containing the validation subset.\n",
    "    \"\"\"\n",
    "    # Validate the size parameter to ensure it is within a proper range\n",
    "    if not 0 < size < 1:\n",
    "        raise ValueError(\"Size parameter must be between 0 and 1 (exclusive)\")\n",
    "\n",
    "    # Step 1: Split the dataset into training and validation sets based on the specified size\n",
    "    train_set, validation_set = dataset.randomSplit([1 - size, size], seed=42)\n",
    "    \n",
    "    # Return only the validation subset\n",
    "    return validation_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize RoBERTa tokenizer and model for multi-class classification\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def compute_metrics(pred) -> dict:\n",
    "    \"\"\"\n",
    "    Computes accuracy and F1 score for model evaluation.\n",
    "    \n",
    "    Parameters:\n",
    "    - pred: Predictions and labels from the model.\n",
    "    \n",
    "    Returns:\n",
    "    - dict: Dictionary containing accuracy and F1 score.\n",
    "    \"\"\"\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "    return {\"accuracy\": acc, \"f1\": f1}\n",
    "\n",
    "def train_small_model(text_column: str) -> float:\n",
    "    \"\"\"\n",
    "    Trains a small RoBERTa model on the specified text composition strategy and \n",
    "    returns the validation performance metric (F1 score).\n",
    "    \n",
    "    Parameters:\n",
    "    - text_column (str): The name of the column in the DataFrame containing the text\n",
    "                         composition strategy to evaluate.\n",
    "    \n",
    "    Returns:\n",
    "    - float: The F1 score on the validation set.\n",
    "    \"\"\"\n",
    "    # Step 1: Tokenize the text data in the column\n",
    "    texts = validation_set.select(text_column).rdd.flatMap(lambda x: x).collect()\n",
    "    labels = validation_set.select(\"label\").rdd.flatMap(lambda x: x).collect()\n",
    "    \n",
    "    encodings = tokenizer(texts, truncation=True, padding=True, max_length=512)\n",
    "    encodings = {key: torch.tensor(val) for key, val in encodings.items()}\n",
    "    labels = torch.tensor(labels)\n",
    "\n",
    "    # Step 2: Create PyTorch Dataset\n",
    "    class TextDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, encodings, labels):\n",
    "            self.encodings = encodings\n",
    "            self.labels = labels\n",
    "            \n",
    "        def __getitem__(self, idx):\n",
    "            item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "            item[\"labels\"] = self.labels[idx]\n",
    "            return item\n",
    "        \n",
    "        def __len__(self):\n",
    "            return len(self.labels)\n",
    "    \n",
    "    validation_dataset = TextDataset(encodings, labels)\n",
    "    \n",
    "    # Step 3: Initialize model for classification with the appropriate number of classes\n",
    "    num_labels = len(set(labels.tolist()))\n",
    "    model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=num_labels)\n",
    "    model.to(device)\n",
    "\n",
    "    # Step 4: Set up Trainer and Training Arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        logging_dir=\"./logs\",\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        num_train_epochs=1,\n",
    "        seed=42,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        eval_dataset=validation_dataset,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    # Step 5: Perform evaluation to get the F1 score\n",
    "    eval_result = trainer.evaluate()\n",
    "    f1_score = eval_result[\"eval_f1\"]\n",
    "\n",
    "    return f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_strategies(validation_set: DataFrame) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compares various text composition strategies by training small models and evaluates\n",
    "    their performance on the validation set.\n",
    "    \n",
    "    Parameters:\n",
    "    - validation_set (DataFrame): Spark DataFrame used for model validation.\n",
    "    \n",
    "    Returns:\n",
    "    - Dict[str, float]: A dictionary with strategy names as keys and their respective\n",
    "                        performance metrics as values.\n",
    "    \"\"\"\n",
    "    # Dictionary to store results for each strategy\n",
    "    strategies_results = {}\n",
    "\n",
    "    # Strategy 1: Only text\n",
    "    validation_set = validation_set.withColumnRenamed(\"plain_text\", \"text_only\")\n",
    "    strategy1_f1 = train_small_model(\"text_only\")\n",
    "    strategies_results[\"Only Text\"] = strategy1_f1\n",
    "\n",
    "    # Strategy 2: Only title\n",
    "    validation_set = validation_set.withColumnRenamed(\"title\", \"title_only\")\n",
    "    strategy2_f1 = train_small_model(\"title_only\")\n",
    "    strategies_results[\"Only Title\"] = strategy2_f1\n",
    "\n",
    "    # Strategy 3: Title + text\n",
    "    validation_set = validation_set.withColumn(\"title_text\", F.concat(F.col(\"title\"), F.lit(\" \"), F.col(\"plain_text\")))\n",
    "    strategy3_f1 = train_small_model(\"title_text\")\n",
    "    strategies_results[\"Title + Text\"] = strategy3_f1\n",
    "\n",
    "    # Strategy 4: Title + [SEP] + text (using pre-existing 'full_text' column)\n",
    "    strategy4_f1 = train_small_model(\"full_text\")\n",
    "    strategies_results[\"Title + [SEP] + Text\"] = strategy4_f1\n",
    "\n",
    "    # Strategy 5: Publisher + Title + Text\n",
    "    validation_set = validation_set.withColumn(\"publisher_title_text\", F.concat(F.col(\"publisher\"), F.lit(\" [SEP] \"), F.col(\"title\"), F.lit(\" [SEP] \"), F.col(\"plain_text\")))\n",
    "    strategy5_f1 = train_small_model(\"publisher_title_text\")\n",
    "    strategies_results[\"Publisher + Title + Text\"] = strategy5_f1\n",
    "\n",
    "    # Strategy 6: Title + Text + Publisher\n",
    "    validation_set = validation_set.withColumn(\"title_text_publisher\", F.concat(F.col(\"title\"), F.lit(\" [SEP] \"), F.col(\"plain_text\"), F.lit(\" [SEP] \"), F.col(\"publisher\")))\n",
    "    strategy6_f1 = train_small_model(\"title_text_publisher\")\n",
    "    strategies_results[\"Title + Text + Publisher\"] = strategy6_f1\n",
    "\n",
    "    # Ablation study: Removing title\n",
    "    validation_set = validation_set.withColumnRenamed(\"plain_text\", \"text_without_title\")\n",
    "    ablation1_f1 = train_small_model(\"text_without_title\")\n",
    "    strategies_results[\"Without Title\"] = ablation1_f1\n",
    "\n",
    "    # Ablation study: Removing publisher\n",
    "    validation_set = validation_set.withColumn(\"text_without_publisher\", F.concat(F.col(\"title\"), F.lit(\" [SEP] \"), F.col(\"plain_text\")))\n",
    "    ablation2_f1 = train_small_model(\"text_without_publisher\")\n",
    "    strategies_results[\"Without Publisher\"] = ablation2_f1\n",
    "\n",
    "    # Ablation study: Without title and publisher\n",
    "    validation_set = validation_set.withColumn(\"text_without_title_publisher\", F.col(\"plain_text\"))\n",
    "    ablation3_f1 = train_small_model(\"text_without_title_publisher\")\n",
    "    strategies_results[\"Without Title & Publisher\"] = ablation3_f1\n",
    "\n",
    "    return strategies_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select Best Strategy - Determines the optimal composition to improve model interpretability and relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_best_strategy(strategies_results: Dict[str, float]) -> str:\n",
    "    \"\"\"\n",
    "    Selects the best performing text composition strategy based on the results from \n",
    "    the validation phase.\n",
    "    \n",
    "    Parameters:\n",
    "    - strategies_results (Dict[str, float]): A dictionary with strategy names as keys\n",
    "                                             and performance metrics (F1 scores) as values.\n",
    "    \n",
    "    Returns:\n",
    "    - str: The name of the best performing strategy.\n",
    "    \"\"\"\n",
    "    # Validate input dictionary to ensure it contains strategies and metrics\n",
    "    if not strategies_results:\n",
    "        raise ValueError(\"The strategies_results dictionary is empty. Ensure it has valid entries.\")\n",
    "    \n",
    "    # Find the strategy with the highest F1 score\n",
    "    best_strategy = max(strategies_results, key=strategies_results.get)\n",
    "    best_score = strategies_results[best_strategy]\n",
    "\n",
    "    # Log or print the best strategy and its score\n",
    "    print(f\"Best Strategy: '{best_strategy}' with an F1 score of {best_score:.4f}\")\n",
    "\n",
    "    return best_strategy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_text_composition(dataset: DataFrame, validation_split: float = 0.1) -> str:\n",
    "    \"\"\"\n",
    "    Validates and selects the best text composition strategy by training small models\n",
    "    on various text composition strategies with a validation set.\n",
    "    \n",
    "    Parameters:\n",
    "    - dataset (DataFrame): Spark DataFrame containing the dataset.\n",
    "    - validation_split (float): The proportion of data to use for validation. Default is 0.1 (10%).\n",
    "    \n",
    "    Returns:\n",
    "    - str: The best performing strategy based on validation metrics.\n",
    "    \"\"\"\n",
    "    # Step 1: Split the dataset into a validation set\n",
    "    validation_set = split_validation_set(dataset, validation_split)\n",
    "\n",
    "    # Step 2: Compare different text composition strategies\n",
    "    strategies_results = compare_strategies(validation_set)\n",
    "\n",
    "    # Step 3: Select the best performing strategy\n",
    "    best_strategy = select_best_strategy(strategies_results)\n",
    "\n",
    "    # Log or print the selected strategy for confirmation\n",
    "    print(f\"Best text composition strategy selected: {best_strategy}\")\n",
    "\n",
    "    return best_strategy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Preparation\n",
    "This step finalizes the dataset for training by encoding categories, tokenizing texts, and splitting into training, validation, and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply Best Composition Strategy - Incorporates the selected composition strategy across the dataset for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_best_text_composition(dataset: DataFrame, strategy: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Applies the specified text composition strategy to the full dataset by transforming\n",
    "    the text columns based on the chosen approach.\n",
    "    \n",
    "    Parameters:\n",
    "    - dataset (DataFrame): Spark DataFrame containing the dataset to transform.\n",
    "    - strategy (str): Text composition strategy to apply, as determined by `validate_text_composition`.\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame: Spark DataFrame with transformed text composition in the 'best_text' column.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the separator token for composition\n",
    "    separator = \" [SEP] \"\n",
    "\n",
    "    # Use the provided strategy to determine text composition\n",
    "    if strategy == \"Only Text\":\n",
    "        # Use only the 'plain_text' column\n",
    "        transformed_dataset = dataset.withColumn(\"best_text\", F.col(\"plain_text\"))\n",
    "    \n",
    "    elif strategy == \"Only Title\":\n",
    "        # Use only the 'title' column\n",
    "        transformed_dataset = dataset.withColumn(\"best_text\", F.col(\"title\"))\n",
    "    \n",
    "    elif strategy == \"Title + Text\":\n",
    "        # Concatenate 'title' and 'plain_text' without separator\n",
    "        transformed_dataset = dataset.withColumn(\"best_text\", F.concat(F.col(\"title\"), F.lit(\" \"), F.col(\"plain_text\")))\n",
    "    \n",
    "    elif strategy == \"Title + [SEP] + Text\":\n",
    "        # Concatenate 'title' and 'plain_text' with \"[SEP]\" separator\n",
    "        transformed_dataset = dataset.withColumn(\"best_text\", F.concat(F.col(\"title\"), F.lit(separator), F.col(\"plain_text\")))\n",
    "    \n",
    "    elif strategy == \"Publisher + Title + Text\":\n",
    "        # Concatenate 'publisher', 'title', and 'plain_text' with \"[SEP]\" separator\n",
    "        transformed_dataset = dataset.withColumn(\n",
    "            \"best_text\",\n",
    "            F.concat(F.col(\"publisher\"), F.lit(separator), F.col(\"title\"), F.lit(separator), F.col(\"plain_text\"))\n",
    "        )\n",
    "    \n",
    "    elif strategy == \"Title + Text + Publisher\":\n",
    "        # Concatenate 'title', 'plain_text', and 'publisher' with \"[SEP]\" separator\n",
    "        transformed_dataset = dataset.withColumn(\n",
    "            \"best_text\",\n",
    "            F.concat(F.col(\"title\"), F.lit(separator), F.col(\"plain_text\"), F.lit(separator), F.col(\"publisher\"))\n",
    "        )\n",
    "    \n",
    "    elif strategy == \"Without Title\":\n",
    "        # Use only 'plain_text', omitting 'title'\n",
    "        transformed_dataset = dataset.withColumn(\"best_text\", F.col(\"plain_text\"))\n",
    "    \n",
    "    elif strategy == \"Without Publisher\":\n",
    "        # Concatenate 'title' and 'plain_text' with \"[SEP]\" separator, omitting 'publisher'\n",
    "        transformed_dataset = dataset.withColumn(\"best_text\", F.concat(F.col(\"title\"), F.lit(separator), F.col(\"plain_text\")))\n",
    "    \n",
    "    elif strategy == \"Without Title & Publisher\":\n",
    "        # Use only 'plain_text', omitting both 'title' and 'publisher'\n",
    "        transformed_dataset = dataset.withColumn(\"best_text\", F.col(\"plain_text\"))\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown strategy: {strategy}. Please provide a valid text composition strategy.\")\n",
    "    \n",
    "    return transformed_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode Categories - Converts categorical labels into numerical format for model compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_categories_labels(categories: List[str]) -> Tuple[List[int], Dict[str, int]]:\n",
    "    \"\"\"\n",
    "    Encodes categories into numerical labels and creates a mapping from category names to \n",
    "    integer labels.\n",
    "    \n",
    "    Parameters:\n",
    "    - categories (List[str]): List of category names to encode.\n",
    "    \n",
    "    Returns:\n",
    "    - Tuple[List[int], Dict[str, int]]: Encoded category labels and a dictionary mapping \n",
    "      category names to integer labels.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Get unique categories and create a mapping from category names to integer labels\n",
    "    unique_categories = sorted(set(categories))\n",
    "    category_mapping = {category: idx for idx, category in enumerate(unique_categories)}\n",
    "\n",
    "    # Step 2: Encode categories using the created mapping\n",
    "    encoded_labels = [category_mapping[category] for category in categories]\n",
    "\n",
    "    return encoded_labels, category_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split Data - Segregates data into training, validation, and test sets, ensuring balanced distribution across categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(dataset: DataFrame, ratios: List[float] = [0.7, 0.15, 0.15], stratify_column: str = \"categories\") -> Tuple[DataFrame, DataFrame, DataFrame]:\n",
    "    \"\"\"\n",
    "    Splits the dataset into training, validation, and test sets according to specified ratios.\n",
    "    \n",
    "    Parameters:\n",
    "    - dataset (DataFrame): Spark DataFrame to split.\n",
    "    - ratios (List[float]): List of ratios for splitting the dataset. Default is [0.7, 0.15, 0.15].\n",
    "    - stratify_column (str): Column name to use for stratification, default is \"categories\".\n",
    "    \n",
    "    Returns:\n",
    "    - Tuple[DataFrame, DataFrame, DataFrame]: A tuple of DataFrames for training, validation, and test sets.\n",
    "    \"\"\"\n",
    "\n",
    "    # Validate ratios to ensure they sum up to 1\n",
    "    if sum(ratios) != 1.0:\n",
    "        raise ValueError(\"Ratios must sum to 1.0\")\n",
    "\n",
    "    # Convert ratios to percentages for stratified splitting\n",
    "    train_ratio, val_ratio, test_ratio = ratios\n",
    "\n",
    "    # Step 1: Add a stratification index column if stratification is required\n",
    "    if stratify_column:\n",
    "        # Generate a unique row ID to handle exact stratification with approximate splits\n",
    "        dataset = dataset.withColumn(\"stratify_id\", F.monotonically_increasing_id())\n",
    "        \n",
    "        # Calculate row numbers within each stratification group\n",
    "        stratified_dataset = dataset.withColumn(\n",
    "            \"stratify_row_num\", \n",
    "            F.row_number().over(Window.partitionBy(stratify_column).orderBy(\"stratify_id\"))\n",
    "        )\n",
    "    else:\n",
    "        stratified_dataset = dataset.withColumn(\"stratify_row_num\", F.monotonically_increasing_id())\n",
    "\n",
    "    # Step 2: Calculate total count for stratification-based splitting\n",
    "    total_rows = stratified_dataset.count()\n",
    "    train_count = int(total_rows * train_ratio)\n",
    "    val_count = int(total_rows * val_ratio)\n",
    "\n",
    "    # Step 3: Split dataset based on stratified row number ranges\n",
    "    train_set = stratified_dataset.filter(F.col(\"stratify_row_num\") <= train_count).drop(\"stratify_row_num\", \"stratify_id\")\n",
    "    val_set = stratified_dataset.filter((F.col(\"stratify_row_num\") > train_count) & (F.col(\"stratify_row_num\") <= train_count + val_count)).drop(\"stratify_row_num\", \"stratify_id\")\n",
    "    test_set = stratified_dataset.filter(F.col(\"stratify_row_num\") > train_count + val_count).drop(\"stratify_row_num\", \"stratify_id\")\n",
    "\n",
    "    return train_set, val_set, test_set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization - Tokenizes each text for compatibility with language models like RoBERTa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenized_dataset(texts: List[str], labels: List[int]) -> List[Tuple[List[int], int]]:\n",
    "    \"\"\"\n",
    "    Tokenizes the input texts and pairs each tokenized text with its corresponding label.\n",
    "    \n",
    "    Parameters:\n",
    "    - texts (List[str]): List of texts to tokenize.\n",
    "    - labels (List[int]): List of labels corresponding to each text.\n",
    "    \n",
    "    Returns:\n",
    "    - List[Tuple[List[int], int]]: A list of tuples where each tuple contains a list of tokenized\n",
    "      IDs for a text and its corresponding label.\n",
    "    \"\"\"\n",
    "    tokenized_data = []\n",
    "\n",
    "    # Ensure texts and labels are of the same length\n",
    "    if len(texts) != len(labels):\n",
    "        raise ValueError(\"The length of texts and labels must be the same.\")\n",
    "\n",
    "    # Tokenize each text and pair it with the corresponding label\n",
    "    for text, label in zip(texts, labels):\n",
    "        # Tokenize text and convert to token IDs\n",
    "        encoding = tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=512,\n",
    "            return_tensors=\"pt\"  # Return PyTorch tensors\n",
    "        )\n",
    "\n",
    "        # Extract token IDs as a list and store with label\n",
    "        token_ids = encoding[\"input_ids\"].squeeze().tolist()  # Convert tensor to list\n",
    "        tokenized_data.append((token_ids, label))\n",
    "\n",
    "    return tokenized_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(cleaned_dataset: DataFrame, best_strategy: str) -> Tuple[List[Tuple[List[int], int]], List[Tuple[List[int], int]], List[Tuple[List[int], int]], Dict[str, int]]:\n",
    "    \"\"\"\n",
    "    Prepares the dataset for model training by applying the best text composition strategy, \n",
    "    removing duplicates, encoding categories, splitting the data, and tokenizing it.\n",
    "    \n",
    "    Parameters:\n",
    "    - cleaned_dataset (DataFrame): Spark DataFrame with cleaned and standardized data.\n",
    "    - best_strategy (str): The best-performing text composition strategy determined through validation.\n",
    "    \n",
    "    Returns:\n",
    "    - Tuple[List[Tuple[List[int], int]], List[Tuple[List[int], int]], List[Tuple[List[int], int]], Dict[str, int]]:\n",
    "      A tuple containing the tokenized training, validation, and test datasets, along with the \n",
    "      category mapping dictionary.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Apply the best text composition strategy to the dataset\n",
    "    dataset_with_best_strategy = apply_best_text_composition(cleaned_dataset, best_strategy)\n",
    "\n",
    "    # Step 2: Extract and clean the categories column for encoding\n",
    "    categories_col = dataset_with_best_strategy.select(\"categories\").rdd.flatMap(lambda x: x).collect()\n",
    "    encoded_labels, category_mapping = encode_categories_labels(categories_col)\n",
    "\n",
    "    # Add encoded labels as a new column to the dataset\n",
    "    dataset_with_best_strategy = dataset_with_best_strategy.withColumn(\"label\", F.array(*[F.lit(label) for label in encoded_labels]))\n",
    "\n",
    "    # Step 3: Split the dataset into training, validation, and test sets\n",
    "    train_set, val_set, test_set = split_data(dataset_with_best_strategy, ratios=[0.7, 0.15, 0.15], stratify_column=\"categories\")\n",
    "\n",
    "    # Step 4: Collect text and labels for each split\n",
    "    train_texts = train_set.select(\"best_text\").rdd.flatMap(lambda x: x).collect()\n",
    "    val_texts = val_set.select(\"best_text\").rdd.flatMap(lambda x: x).collect()\n",
    "    test_texts = test_set.select(\"best_text\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "    train_labels = train_set.select(\"label\").rdd.flatMap(lambda x: x).collect()\n",
    "    val_labels = val_set.select(\"label\").rdd.flatMap(lambda x: x).collect()\n",
    "    test_labels = test_set.select(\"label\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "    # Step 5: Tokenize texts for each dataset split\n",
    "    train_dataset = create_tokenized_dataset(train_texts, train_labels)\n",
    "    val_dataset = create_tokenized_dataset(val_texts, val_labels)\n",
    "    test_dataset = create_tokenized_dataset(test_texts, test_labels)\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset, category_mapping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. MODEL INITIALIZATION\n",
    "Loads a pre-trained RoBERTa model with a classification layer, tailored for multi-class classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(num_categories: int) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Initializes a RoBERTa model with a classification head for multi-class classification.\n",
    "    \n",
    "    Parameters:\n",
    "    - num_categories (int): The number of unique categories for classification.\n",
    "    \n",
    "    Returns:\n",
    "    - nn.Module: A PyTorch model with a RoBERTa backbone and a classification head.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load the pre-trained RoBERTa model with a classification head\n",
    "    model = RobertaForSequenceClassification.from_pretrained(\n",
    "        \"roberta-large\",\n",
    "        num_labels=num_categories  # Set the number of output labels for classification\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Hyperparameter Tuning\n",
    "Fine-tunes model hyperparameters using Bayesian Optimization via Optuna to achieve optimal training configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_hyperparameters(train_dataset: Dataset, validation_dataset: Dataset, num_categories: int) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Tunes hyperparameters for the RoBERTa model using Bayesian Optimization with Optuna.\n",
    "    \n",
    "    Parameters:\n",
    "    - train_dataset (Dataset): The training dataset to use for hyperparameter tuning.\n",
    "    - validation_dataset (Dataset): The validation dataset to use for evaluating performance.\n",
    "    - num_categories (int): The number of unique categories for classification.\n",
    "    \n",
    "    Returns:\n",
    "    - Dict[str, Any]: A dictionary containing the best hyperparameters found during tuning.\n",
    "    \"\"\"\n",
    "\n",
    "    def model_init() -> nn.Module:\n",
    "        \"\"\"\n",
    "        Initializes the RoBERTa model with a classification head based on the number of categories.\n",
    "        \n",
    "        Returns:\n",
    "        - nn.Module: A PyTorch model instance ready for training.\n",
    "        \"\"\"\n",
    "        return RobertaForSequenceClassification.from_pretrained(\"roberta-large\", num_labels=num_categories)\n",
    "\n",
    "    def objective(trial) -> float:\n",
    "        \"\"\"\n",
    "        Objective function for Bayesian Optimization, which defines the search space and evaluates \n",
    "        each hyperparameter combination.\n",
    "        \n",
    "        Parameters:\n",
    "        - trial: An Optuna trial object that suggests hyperparameter values.\n",
    "        \n",
    "        Returns:\n",
    "        - float: Validation loss for the current trial’s hyperparameters.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Define the hyperparameter search space\n",
    "        learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 5e-5, log=True)\n",
    "        batch_size = trial.suggest_categorical(\"batch_size\", [16, 32])\n",
    "        epochs = trial.suggest_int(\"epochs\", 3, 10)\n",
    "        warmup_steps = trial.suggest_int(\"warmup_steps\", 0, 1000)\n",
    "        dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.5)\n",
    "        \n",
    "        # Set up training arguments with current trial’s hyperparameters\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=\"./results\",\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            learning_rate=learning_rate,\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            num_train_epochs=epochs,\n",
    "            warmup_steps=warmup_steps,\n",
    "            weight_decay=dropout_rate,\n",
    "            logging_dir='./logs',\n",
    "            logging_steps=10\n",
    "        )\n",
    "\n",
    "        # Initialize the Trainer with current hyperparameters\n",
    "        trainer = Trainer(\n",
    "            model_init=model_init,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=validation_dataset\n",
    "        )\n",
    "\n",
    "        # Run training and return the validation loss for evaluation\n",
    "        result = trainer.evaluate()\n",
    "        return result[\"eval_loss\"]\n",
    "\n",
    "    # Create an Optuna study to perform Bayesian Optimization\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=20)  # Run for a fixed number of trials (20 here)\n",
    "\n",
    "    # Extract the best hyperparameters\n",
    "    best_hyperparameters = study.best_params\n",
    "    print(\"Best Hyperparameters:\", best_hyperparameters)\n",
    "\n",
    "    return best_hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. TRAINING MODEL\n",
    "Conducts the model training, including early stopping to prevent overfitting and achieve efficient convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model: nn.Module,\n",
    "    train_dataset: List[Tuple[List[int], int]],\n",
    "    validation_dataset: List[Tuple[List[int], int]],\n",
    "    max_epochs: int,\n",
    "    batch_size: int,\n",
    "    learning_rate: float,\n",
    "    patience: int = 3\n",
    ") -> nn.Module:\n",
    "    \"\"\"\n",
    "    Trains the RoBERTa model on the provided training dataset and validates on the validation dataset,\n",
    "    implementing early stopping with checkpointing based on validation loss.\n",
    "    \n",
    "    Parameters:\n",
    "    - model (nn.Module): The PyTorch model to be trained.\n",
    "    - train_dataset (List[Tuple[List[int], int]]): Tokenized training dataset with labels.\n",
    "    - validation_dataset (List[Tuple[List[int], int]]): Tokenized validation dataset with labels.\n",
    "    - max_epochs (int): Maximum number of training epochs.\n",
    "    - batch_size (int): Batch size for training.\n",
    "    - learning_rate (float): Learning rate for optimization.\n",
    "    - patience (int): Patience for early stopping. Default is 3.\n",
    "    \n",
    "    Returns:\n",
    "    - nn.Module: The trained PyTorch model.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Prepare DataLoader for training and validation datasets\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(validation_dataset, batch_size=batch_size)\n",
    "    \n",
    "    # Initialize optimizer\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Training and early stopping parameters\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    # Move model to device (GPU if available)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        total_train = 0\n",
    "\n",
    "        # Training phase\n",
    "        for batch in train_loader:\n",
    "            input_ids = batch[0].to(device)\n",
    "            labels = batch[1].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(input_ids=input_ids, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            predictions = outputs.logits.argmax(dim=1)\n",
    "            train_correct += (predictions == labels).sum().item()\n",
    "            total_train += labels.size(0)\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        train_accuracy = train_correct / total_train\n",
    "        print(f\"Epoch {epoch + 1}/{max_epochs}, Training Loss: {avg_train_loss:.4f}, Training Accuracy: {train_accuracy:.4f}\")\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        total_val = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch[0].to(device)\n",
    "                labels = batch[1].to(device)\n",
    "\n",
    "                outputs = model(input_ids=input_ids, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                predictions = outputs.logits.argmax(dim=1)\n",
    "                val_correct += (predictions == labels).sum().item()\n",
    "                total_val += labels.size(0)\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_accuracy = val_correct / total_val\n",
    "        print(f\"Epoch {epoch + 1}/{max_epochs}, Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "        # Checkpointing and early stopping check\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), \"best_model.pt\")  # Save the best model checkpoint\n",
    "            print(\"Validation loss improved, saving model checkpoint.\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"No improvement in validation loss. Patience counter: {patience_counter}\")\n",
    "\n",
    "        # Stop training if patience limit is reached\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "    # Load the best model before returning\n",
    "    model.load_state_dict(torch.load(\"best_model.pt\"))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. EVALUATING MODEL\n",
    "Assesses model performance on the test dataset, generating accuracy, confusion matrix, and precision-recall metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(\n",
    "    model: nn.Module,\n",
    "    test_dataset: List[Tuple[List[int], int]],\n",
    "    num_categories: int\n",
    ") -> Tuple[Dict[str, float], List[List[int]], float, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Evaluates the trained model on the test dataset, calculating predictions, \n",
    "    confusion matrix, accuracy, precision, recall, F1 scores, and performs k-fold cross-validation.\n",
    "    \n",
    "    Parameters:\n",
    "    - model (nn.Module): The trained PyTorch model to be evaluated.\n",
    "    - test_dataset (List[Tuple[List[int], int]]): Tokenized test dataset with labels.\n",
    "    - num_categories (int): The number of unique categories for classification.\n",
    "    \n",
    "    Returns:\n",
    "    - Tuple[Dict[str, float], List[List[int]], float, Dict[str, float]]: \n",
    "        Dictionary with precision, recall, F1 scores for each class, confusion matrix, \n",
    "        overall accuracy, and k-fold cross-validation metrics.\n",
    "    \"\"\"\n",
    "    \n",
    "    # DataLoader for test dataset\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "    \n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    # No gradient calculation needed during evaluation\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch[0].to(device)\n",
    "            labels = batch[1].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(input_ids=input_ids)\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            \n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "    # Calculate metrics\n",
    "    confusion = confusion_matrix(all_labels, all_preds)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average=\"weighted\")\n",
    "    \n",
    "    per_class_metrics = {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"accuracy\": accuracy\n",
    "    }\n",
    "    \n",
    "    # Print per-category metrics\n",
    "    for category in range(num_categories):\n",
    "        cat_precision, cat_recall, cat_f1, _ = precision_recall_fscore_support(\n",
    "            all_labels, all_preds, labels=[category], average=\"binary\", zero_division=0\n",
    "        )\n",
    "        print(f\"Category {category} - Precision: {cat_precision:.4f}, Recall: {cat_recall:.4f}, F1: {cat_f1:.4f}\")\n",
    "    \n",
    "    # K-Fold Cross-Validation\n",
    "    kfold_metrics = {}\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    fold_accuracies = []\n",
    "    for train_idx, val_idx in kf.split(test_dataset):\n",
    "        train_subset = [test_dataset[i] for i in train_idx]\n",
    "        val_subset = [test_dataset[i] for i in val_idx]\n",
    "        \n",
    "        train_loader = DataLoader(train_subset, batch_size=32)\n",
    "        val_loader = DataLoader(val_subset, batch_size=32)\n",
    "        \n",
    "        model.train()\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=0.00002)\n",
    "        \n",
    "        for epoch in range(1):  # Training for one epoch\n",
    "            for batch in train_loader:\n",
    "                input_ids = batch[0].to(device)\n",
    "                labels = batch[1].to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(input_ids=input_ids, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        # Evaluation on validation set for the fold\n",
    "        val_labels, val_preds = [], []\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch[0].to(device)\n",
    "                labels = batch[1].to(device)\n",
    "\n",
    "                outputs = model(input_ids=input_ids)\n",
    "                preds = torch.argmax(outputs.logits, dim=1)\n",
    "\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "        fold_accuracy = accuracy_score(val_labels, val_preds)\n",
    "        fold_accuracies.append(fold_accuracy)\n",
    "\n",
    "    kfold_metrics[\"kfold_accuracy_mean\"] = np.mean(fold_accuracies)\n",
    "    kfold_metrics[\"kfold_accuracy_std\"] = np.std(fold_accuracies)\n",
    "\n",
    "    return per_class_metrics, confusion.tolist(), accuracy, kfold_metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAVE RESULTS\n",
    "Persistently stores the trained model, evaluation metrics, and category mappings, ensuring reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model: nn.Module, file_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Saves the trained model to the specified file path in PyTorch's native format.\n",
    "\n",
    "    Parameters:\n",
    "    - model (nn.Module): Model to save, typically a PyTorch-based transformer model.\n",
    "    - file_path (str): Path to save the model (should end with .pth or .pt).\n",
    "    \"\"\"\n",
    "    # Ensure the model is in evaluation mode before saving\n",
    "    model.eval()\n",
    "    \n",
    "    # Save the model's state dictionary to the specified file path\n",
    "    torch.save(model.state_dict(), file_path)\n",
    "    \n",
    "    print(f\"Model saved successfully at {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_metrics(metrics: Dict[str, Any], file_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Saves evaluation metrics to a specified file path in JSON format.\n",
    "\n",
    "    Parameters:\n",
    "    - metrics (Dict[str, Any]): Dictionary containing the evaluation metrics to save.\n",
    "    - file_path (str): Path to save the metrics (should end with .json).\n",
    "    \"\"\"\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "    \n",
    "    print(f\"Metrics saved successfully at {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_category_mapping(mapping: Dict[str, int], file_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Saves category mapping to a specified file path in JSON format.\n",
    "\n",
    "    Parameters:\n",
    "    - mapping (Dict[str, int]): Mapping of categories to integer labels.\n",
    "    - file_path (str): Path to save the category mapping (should end with .json).\n",
    "    \"\"\"\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(mapping, f, indent=4)\n",
    "    \n",
    "    print(f\"Category mapping saved successfully at {file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN EXECUTION FLOW\n",
    "The main procedure orchestrates the entire workflow, from data loading and preprocessing to model training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main() -> None:\n",
    "    \"\"\"\n",
    "    Main execution flow for news classification, including data loading, preprocessing, \n",
    "    model initialization, hyperparameter tuning, training, evaluation, and saving results.\n",
    "    \"\"\"\n",
    "    # Step 1: Load and preprocess\n",
    "    raw_data = load_dataset(\"data.parquet\")\n",
    "    cleaned_data = preprocess_data(raw_data)\n",
    "    \n",
    "    # Step 2: Validate text composition\n",
    "    best_strategy = validate_text_composition(cleaned_data)\n",
    "    \n",
    "    # Step 3: Prepare data\n",
    "    train_loader, validation_loader, test_loader, category_mapping = prepare_data(cleaned_data, best_strategy)\n",
    "    \n",
    "    # Step 4: Hyperparameter tuning\n",
    "    best_hyperparameters = tune_hyperparameters(\n",
    "        train_dataset=train_loader,\n",
    "        validation_dataset=validation_loader,\n",
    "        num_categories=len(category_mapping)\n",
    "    )\n",
    "\n",
    "    # Step 5: Initialize model with the correct number of categories\n",
    "    model = initialize_model(num_categories=len(category_mapping))\n",
    "    \n",
    "    # Step 6: Train with tuned hyperparameters\n",
    "    trained_model = train_model(\n",
    "        model=model,\n",
    "        train_dataset=train_loader,\n",
    "        validation_dataset=validation_loader,\n",
    "        max_epochs=best_hyperparameters[\"epochs\"],\n",
    "        batch_size=best_hyperparameters[\"batch_size\"],\n",
    "        learning_rate=best_hyperparameters[\"learning_rate\"],\n",
    "        patience=3  # Optional: Adjust if needed based on results\n",
    "    )\n",
    "    \n",
    "    # Step 7: Evaluate on the test set\n",
    "    metrics = evaluate_model(trained_model, test_loader, num_categories=len(category_mapping))\n",
    "    \n",
    "    # Step 8: Save results\n",
    "    save_model(trained_model, \"final_model.pth\")\n",
    "    save_metrics(metrics, \"evaluation_metrics.json\")\n",
    "    save_category_mapping(category_mapping, \"category_mapping.json\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
